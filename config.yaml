logging:
  level: INFO
  format: "%(asctime)-15s %(levelname)-8s %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"

simulation:
  verbose: true
  synth_dataset: false
  f_sel: balanced_accuracy_score
  ecoc_enc: 'sparse' # complete # 'OVA' # 'dense' # 'OVO' # 
  flg_swp: true
  maj_min: false
  min_maj: false
  n_simus: 5
  test_size: 20
  N_MAX: 25000
  num_folds: 5
  model_selection: "avg"

paths:
  data_folder: datasets
  output_folder: results_test

models:
  - name: LogisticRegression
    class: sklearn.linear_model.LogisticRegression
    params:
      solver: lbfgs
      max_iter: 1000
    dynamic_params:
      LR_C: [0.01, 0.1, 1, 10, 100, 1000]
      LR_penalty: ["l2"]

  - name: RandomForestClassifier
    class: sklearn.ensemble.RandomForestClassifier
    params:
      criterion: gini
      n_jobs: -1
      class_weight: balanced
    dynamic_params:
      RF_n_estimators: [20, 40, 60]
      RF_max_depth: [null]
      RF_min_samples_split: [2, 5]
      RF_min_samples_leaf: [1, 5]

  - name: LSEnsemble
    class: libraries.labelswitching.LSEnsemble
    params:
      base_learner: FAMLP
      optim: lbfgs
      activation_fn: tanh
      loss_fn: MSE
    dynamic_params:
      LS_alpha: [0, 0.1, 0.2, 0.3, 0.4]
      LS_beta: [0, 0.05, 0.1]
      LS_Q_C: [1, 0.5, 0.33333, 0.25, 0.2]
      LS_Q_RB_S: [1, 2, 4]
      LS_Q_RB_C: [1, 2, 3, 4, 5, 6, 7, 8]
      LS_num_experts: [11]
      LS_hidden_size: [4]
      LS_drop_out: [0.0]
      LS_n_batch: ["auto"]
      LS_n_epoch: [1]
      LS_mode: ["random"]
    LSE_optimization:
      SW: true
      QC: true
      RI_C: false
      RI_P: false

  - name: MLPClassifier
    class: sklearn.neural_network.MLPClassifier
    params:
      max_iter: 500
      random_state: 42
      tol: 0.0001
    dynamic_params:
      MLP_hidden_layer_sizes: [[20], [40], [60]]
      MLP_activation: ["relu", "tanh", "logistic"]
      MLP_solver: ["adam"]
      MLP_alpha: [0.0001, 0.001]

  - name: LGBMClassifier
    class: lightgbm.LGBMClassifier
    params:
      boosting_type: gbdt
      max_depth: -1
      force_col_wise: true
      verbosity: -1
      is_unbalance: true
    dynamic_params:
      LGBM_num_leaves: [31]
      LGBM_learning_rate: [0.1, 0.05]
      LGBM_n_estimators: [20, 40, 60]

  - name: kNN
    class: sklearn.neighbors.KNeighborsClassifier
    params:
      algorithm: auto
      weights: uniform
    dynamic_params:
      kNN_n_neighbors: [3, 5, 7, 9]  # Common values for k
      kNN_metric: [euclidean, manhattan, minkowski]  # Common distance metrics

  - name: C4.5
    class: sklearn.tree.DecisionTreeClassifier
    params:
      criterion: entropy  # C4.5 uses entropy-based splitting
      splitter: best
    dynamic_params:
      C45_max_depth: [None, 5, 10, 20]  # Limits tree depth for regularization
      C45_min_samples_split: [2, 5, 10]  # Minimum samples needed to split a node
      C45_min_samples_leaf: [1, 2, 5]  # Minimum samples in a leaf node

  - name: SVM
    class: sklearn.svm.SVC
    params:
      kernel: rbf
      probability: true  # Enables probability estimation
    dynamic_params:
      SVM_C: [0.01, 0.1, 1, 10, 100, 1000]  # Regularization parameter
      SVM_gamma: [scale, auto, 0.01, 0.1, 1]  # Kernel coefficient